{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KsEFmBFssNs"
      },
      "source": [
        "# BASELINE FOR MEME CLASSIFIER\n",
        "\n",
        "Sources:\n",
        "\n",
        "\n",
        "*   https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5\n",
        "*   https://stackabuse.com/text-classification-with-python-and-scikit-learn/\n",
        "*   https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f\n",
        "*   https://towardsdatascience.com/named-entity-recognition-and-classification-with-scikit-learn-f05372f07ba2 (task2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO4a3weHOzd7"
      },
      "source": [
        "### DRIVE LINKING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRVeig4uO2rV",
        "outputId": "4d8d7eb5-39cb-46b9-c151-3c770adafc22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CONSTANTS"
      ],
      "metadata": {
        "id": "WhiCUghlw8Ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LABELS = {\n",
        "    'Black-and-white Fallacy/Dictatorship': 0,\n",
        "    'Loaded Language': 1, \n",
        "    'Name calling/Labeling': 2, \n",
        "    'Slogans': 3, \n",
        "    'Smears': 4, \n",
        "    'Causal Oversimplification': 5, \n",
        "    'Appeal to fear/prejudice': 6, \n",
        "    'Exaggeration/Minimisation': 7, \n",
        "    'Reductio ad hitlerum': 8, \n",
        "    'Repetition': 9, \n",
        "    'Glittering generalities (Virtue)': 10, \n",
        "    \"Misrepresentation of Someone's Position (Straw Man)\": 11, \n",
        "    'Doubt': 12, \n",
        "    'Obfuscation, Intentional vagueness, Confusion': 13, \n",
        "    'Whataboutism': 14, \n",
        "    'Flag-waving': 15, \n",
        "    'Thought-terminating clich√©': 16, \n",
        "    'Presenting Irrelevant Data (Red Herring)': 17, \n",
        "    'Appeal to authority': 18, \n",
        "    'Bandwagon': 19,\n",
        "}\n",
        "N_LABELS = len(LABELS)"
      ],
      "metadata": {
        "id": "DrYL0QqNw_Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDpu3qtRulP6"
      },
      "source": [
        "### IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import json\n",
        "\n",
        "import re\n",
        "\n",
        "from tabulate import tabulate\n",
        "import seaborn as sn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import pickle\n",
        "\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qRDAj2wOsCk",
        "outputId": "c964fa57-a566-4751-a2ad-5d6a0d869987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TASK 1"
      ],
      "metadata": {
        "id": "Rfi9uRLJGp24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DATASET"
      ],
      "metadata": {
        "id": "SvHw7XRR4d_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_datasets(task_number):\n",
        "\n",
        "    folder = \"drive/MyDrive/DeepLearning/Dataset/\"\n",
        "\n",
        "    if   task_number == 1:\n",
        "        train_path = folder + \"task1/training_set_task1.txt\"\n",
        "        test_path = folder + \"task1/test_set_task1.txt\"\n",
        "        dev_path = folder + \"task1/dev_set_task1.txt\"\n",
        "    elif task_number == 2:\n",
        "        train_path = folder + \"task2/training_set_task2.txt\"\n",
        "        test_path = folder + \"task2/test_set_task2.txt\"\n",
        "        dev_path = folder + \"task2/dev_set_task2.txt\"\n",
        "    elif task_number == 3:\n",
        "        train_path = folder + \"task3/training/training_set_task3.txt\"\n",
        "        test_path = folder + \"task3/test/test_set_task3.txt\"\n",
        "        dev_path = folder + \"task3/dev/dev_set_task3.txt\"\n",
        "    else:\n",
        "        raise AssertionError('Bad task_number. Possible inputs: 1, 2, 3.')\n",
        "\n",
        "    def load_json2pandas(path): \n",
        "        \n",
        "        with open(path) as f:\n",
        "            data = json.load(f)\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    train = load_json2pandas(train_path)\n",
        "    test = load_json2pandas(test_path)\n",
        "    dev = load_json2pandas(dev_path)\n",
        "\n",
        "    return train, test, dev"
      ],
      "metadata": {
        "id": "4Bk3bCyPQ6no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoder(labels):\n",
        "\n",
        "    def encode(labels):\n",
        "\n",
        "        ohe_label = [0] * N_LABELS\n",
        "        for l in labels:\n",
        "            ohe_label[LABELS[l]] = 1\n",
        "\n",
        "        return ohe_label\n",
        "\n",
        "    return np.array([encode(l) for l in labels])"
      ],
      "metadata": {
        "id": "JaQOQrl2Tm8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_sentences(sentences):\n",
        "\n",
        "    stemmer = WordNetLemmatizer()\n",
        "\n",
        "    def process_sentence(sentence):\n",
        "\n",
        "        # Remove all the special characters\n",
        "        document = re.sub(r'\\W', ' ', str(sentence))\n",
        "        \n",
        "        # remove all single characters\n",
        "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "        \n",
        "        # Remove single characters from the start\n",
        "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "        \n",
        "        # Substituting multiple spaces with single space\n",
        "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "        \n",
        "        # Removing prefixed 'b'\n",
        "        document = re.sub(r'^b\\s+', '', document)\n",
        "        \n",
        "        # Converting to Lowercase\n",
        "        document = document.lower()\n",
        "        \n",
        "        # Lemmatization\n",
        "        document = document.split()\n",
        "\n",
        "        document = [stemmer.lemmatize(word) for word in document]\n",
        "        document = ' '.join(document)\n",
        "\n",
        "        return document\n",
        "\n",
        "    return [process_sentence(sen) for sen in sentences]"
      ],
      "metadata": {
        "id": "zjdY9QNOP2Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataset(train, test, dev):\n",
        "\n",
        "    documents = np.concatenate([\n",
        "        process_sentences(train.text),\n",
        "        process_sentences(test.text),\n",
        "        process_sentences(dev.text)])\n",
        "\n",
        "    tfidf = TfidfVectorizer(sublinear_tf=True, \n",
        "                        min_df=5, \n",
        "                        max_df=0.7,\n",
        "                        stop_words='english')\n",
        "    \n",
        "    X = tfidf.fit_transform(documents).toarray()\n",
        "\n",
        "    X_train = X[:len(train)]\n",
        "    X_test = X[len(train):len(train)+len(test)]\n",
        "    X_dev = X[len(train)+len(test):]\n",
        "\n",
        "    y_train = one_hot_encoder(train.labels)\n",
        "    y_test = one_hot_encoder(test.labels)\n",
        "    y_dev = one_hot_encoder(dev.labels)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, X_dev, y_dev"
      ],
      "metadata": {
        "id": "jQYxd1ZUW-mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### VISUALIZING DATA"
      ],
      "metadata": {
        "id": "q9sD6Rck98me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_labels_histo(ds):\n",
        "\n",
        "    labels = [LABELS[l] for l_list in ds.labels for l in l_list]\n",
        "\n",
        "    plt.hist(labels, bins=40)\n",
        "    plt.xticks(range(0, max(labels) + 1))\n",
        "    plt.show()\n",
        "\n",
        "train, _, _ = load_datasets(1)\n",
        "plot_labels_histo(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3FLDe-n-ALz",
        "outputId": "8028e06b-aa39-457c-b914-3df8f06c5c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUuElEQVR4nO3df/BddX3n8edLAohoBeQrG5O4YW20pc4aMKV0tZZCtYAdg93KwHSVKp20XdgF120LutPqdpnBVmXXmV06UahoEWUVC1Xagoh1nVnAgAESQI0SStJAvvUH4jKLBd77x/1kvYTvj3vu/X4Tcnw+Zu7ccz7nvM/53G/Ofd1zP/fcm1QVkqR+edbe7oAkaeEZ7pLUQ4a7JPWQ4S5JPWS4S1IPLdnbHQA4/PDDa+XKlXu7G5K0T7ntttv+saqmZlr2jAj3lStXsmHDhr3dDUnapyS5f7ZlDstIUg8Z7pLUQ4a7JPWQ4S5JPWS4S1IPGe6S1EOGuyT1kOEuST1kuEtSDz0jvqG6N608/3OzLtt60ev3YE8kaeHMe+ae5NlJbk1yR5LNSd7T2j+S5L4kG9ttdWtPkg8m2ZLkziTHLPaDkCQ91Shn7o8BJ1TVD5LsD3w5yV+3Zb9XVZ/abf2TgVXt9nPAJe1ekrSHzHvmXgM/aLP7t9tc//HqWuCjre5m4JAkSyfvqiRpVCN9oJpkvyQbgZ3ADVV1S1t0YRt6uTjJga1tGfDAUPm21rb7Ntcl2ZBkw/T09AQPQZK0u5HCvaqeqKrVwHLg2CQvBy4Afgr4WeAw4A+67Liq1lfVmqpaMzU1488RS5LG1OlSyKr6HnATcFJV7WhDL48Bfw4c21bbDqwYKlve2iRJe8goV8tMJTmkTR8EvBa4d9c4epIApwKbWsm1wFvaVTPHAQ9X1Y5F6b0kaUajXC2zFLg8yX4MXgyuqqrPJvlCkikgwEbgd9r61wGnAFuAR4G3Lny3JUlzmTfcq+pO4OgZ2k+YZf0Czp68a5KkcfnzA5LUQ4a7JPWQ4S5JPWS4S1IPGe6S1EOGuyT1kOEuST1kuEtSDxnuktRDhrsk9ZDhLkk9ZLhLUg8Z7pLUQ4a7JPWQ4S5JPWS4S1IPGe6S1EOGuyT1kOEuST1kuEtSD80b7kmeneTWJHck2ZzkPa39yCS3JNmS5JNJDmjtB7b5LW35ysV9CJKk3Y1y5v4YcEJVvQJYDZyU5DjgvcDFVfWTwHeBs9r6ZwHfbe0Xt/UkSXvQvOFeAz9os/u3WwEnAJ9q7ZcDp7bptW2etvzEJFmwHkuS5jXSmHuS/ZJsBHYCNwDfBL5XVY+3VbYBy9r0MuABgLb8YeAFM2xzXZINSTZMT09P9igkSU8xUrhX1RNVtRpYDhwL/NSkO66q9VW1pqrWTE1NTbo5SdKQTlfLVNX3gJuAnwcOSbKkLVoObG/T24EVAG3584FvL0hvJUkjGeVqmakkh7Tpg4DXAvcwCPlfb6udCVzTpq9t87TlX6iqWshOS5LmtmT+VVgKXJ5kPwYvBldV1WeT3A18Isl/Ab4KXNrWvxT4WJItwHeA0xeh35KkOcwb7lV1J3D0DO3fYjD+vnv7/wXetCC9kySNxW+oSlIPGe6S1EOGuyT1kOEuST1kuEtSDxnuktRDhrsk9ZDhLkk9ZLhLUg8Z7pLUQ4a7JPWQ4S5JPWS4S1IPGe6S1EOGuyT1kOEuST1kuEtSDxnuktRDhrsk9ZDhLkk9NG+4J1mR5KYkdyfZnOTc1v7uJNuTbGy3U4ZqLkiyJcnXkvzKYj4ASdLTLRlhnceBd1TV7UmeB9yW5Ia27OKqet/wykmOAk4HfgZ4EfD5JC+tqicWsuOSpNnNe+ZeVTuq6vY2/QhwD7BsjpK1wCeq6rGqug/YAhy7EJ2VJI2m05h7kpXA0cAtremcJHcmuSzJoa1tGfDAUNk2ZngxSLIuyYYkG6anpzt3XJI0u5HDPclzgU8D51XV94FLgJcAq4EdwPu77Liq1lfVmqpaMzU11aVUkjSPkcI9yf4Mgv2KqroaoKoeqqonqupJ4EP8aOhlO7BiqHx5a5Mk7SGjXC0T4FLgnqr6wFD70qHV3ghsatPXAqcnOTDJkcAq4NaF67IkaT6jXC3zKuDNwF1JNra2dwJnJFkNFLAV+G2Aqtqc5CrgbgZX2pztlTKStGfNG+5V9WUgMyy6bo6aC4ELJ+iXJGkCfkNVknrIcJekHjLcJamHDHdJ6iHDXZJ6yHCXpB4y3CWphwx3Seohw12Seshwl6QeMtwlqYcMd0nqIcNdknpolJ/81SxWnv+5WZdtvej1e7AnkvRUnrlLUg8Z7pLUQ4a7JPWQ4S5JPWS4S1IPGe6S1EPzhnuSFUluSnJ3ks1Jzm3thyW5Ick32v2hrT1JPphkS5I7kxyz2A9CkvRUo5y5Pw68o6qOAo4Dzk5yFHA+cGNVrQJubPMAJwOr2m0dcMmC91qSNKd5w72qdlTV7W36EeAeYBmwFri8rXY5cGqbXgt8tAZuBg5JsnTBey5JmlWnb6gmWQkcDdwCHFFVO9qiB4Ej2vQy4IGhsm2tbcdQG0nWMTiz58UvfnHHbv94m+ubseC3YyV1+EA1yXOBTwPnVdX3h5dVVQHVZcdVtb6q1lTVmqmpqS6lkqR5jBTuSfZnEOxXVNXVrfmhXcMt7X5na98OrBgqX97aJEl7yChXywS4FLinqj4wtOha4Mw2fSZwzVD7W9pVM8cBDw8N30iS9oBRxtxfBbwZuCvJxtb2TuAi4KokZwH3A6e1ZdcBpwBbgEeBty5ojyVJ85o33Kvqy0BmWXziDOsXcPaE/ZIkTcBvqEpSDxnuktRDhrsk9ZDhLkk9ZLhLUg8Z7pLUQ4a7JPWQ4S5JPWS4S1IPGe6S1EOGuyT1kOEuST1kuEtSDxnuktRDhrsk9ZDhLkk9ZLhLUg8Z7pLUQ4a7JPWQ4S5JPTRvuCe5LMnOJJuG2t6dZHuSje12ytCyC5JsSfK1JL+yWB2XJM1ulDP3jwAnzdB+cVWtbrfrAJIcBZwO/Eyr+R9J9luozkqSRjNvuFfVl4DvjLi9tcAnquqxqroP2AIcO0H/JEljmGTM/Zwkd7Zhm0Nb2zLggaF1trW2p0myLsmGJBump6cn6IYkaXfjhvslwEuA1cAO4P1dN1BV66tqTVWtmZqaGrMbkqSZjBXuVfVQVT1RVU8CH+JHQy/bgRVDqy5vbZKkPWiscE+ydGj2jcCuK2muBU5PcmCSI4FVwK2TdVGS1NWS+VZIciVwPHB4km3AHwHHJ1kNFLAV+G2Aqtqc5CrgbuBx4OyqemJxui5Jms284V5VZ8zQfOkc618IXDhJpyRJk/EbqpLUQ4a7JPWQ4S5JPWS4S1IPGe6S1EOGuyT1kOEuST1kuEtSDxnuktRDhrsk9ZDhLkk9ZLhLUg8Z7pLUQ4a7JPWQ4S5JPWS4S1IPGe6S1EOGuyT1kOEuST1kuEtSD80b7kkuS7IzyaahtsOS3JDkG+3+0NaeJB9MsiXJnUmOWczOS5JmNsqZ+0eAk3ZrOx+4sapWATe2eYCTgVXttg64ZGG6KUnqYt5wr6ovAd/ZrXktcHmbvhw4daj9ozVwM3BIkqUL1VlJ0mjGHXM/oqp2tOkHgSPa9DLggaH1trW2p0myLsmGJBump6fH7IYkaSYTf6BaVQXUGHXrq2pNVa2ZmpqatBuSpCHjhvtDu4Zb2v3O1r4dWDG03vLWJknag8YN92uBM9v0mcA1Q+1vaVfNHAc8PDR8I0naQ5bMt0KSK4HjgcOTbAP+CLgIuCrJWcD9wGlt9euAU4AtwKPAWxehz5Kkecwb7lV1xiyLTpxh3QLOnrRTkqTJ+A1VSeohw12Seshwl6QeMtwlqYcMd0nqIcNdknrIcJekHjLcJamHDHdJ6iHDXZJ6yHCXpB4y3CWphwx3Seohw12Seshwl6QeMtwlqYcMd0nqIcNdknrIcJekHjLcJamH5v0PsueSZCvwCPAE8HhVrUlyGPBJYCWwFTitqr47WTclSV0sxJn7L1XV6qpa0+bPB26sqlXAjW1ekrQHTXTmPou1wPFt+nLgi8AfLMJ+tI9Zef7n5ly+9aLX76GeSP036Zl7AdcnuS3JutZ2RFXtaNMPAkfMVJhkXZINSTZMT09P2A1J0rBJz9xfXVXbk7wQuCHJvcMLq6qS1EyFVbUeWA+wZs2aGdeRNLu53gn5LkgTnblX1fZ2vxP4DHAs8FCSpQDtfueknZQkdTP2mXuSg4FnVdUjbfp1wH8GrgXOBC5q99csREf7xvFnSYtpkmGZI4DPJNm1nY9X1d8k+QpwVZKzgPuB0ybvpp4pHAqQ9g1jh3tVfQt4xQzt3wZOnKRTkqTJLMalkJJ6zCHFfYM/PyBJPWS4S1IPGe6S1EOOuUsTcPxZz1SeuUtSDxnuktRDDsv8GPKLSNpbHMbaczxzl6QeMtwlqYcMd0nqIcfcpb3E8efu/LxodPt8uPsEkaSnc1hGknrIcJekHtrnh2UkabHti8O/nrlLUg8Z7pLUQw7LaJ8wydviffEttTQpz9wlqYcW7cw9yUnAfwP2Az5cVRct1r4kdeOXgfpvUc7ck+wH/HfgZOAo4IwkRy3GviRJT7dYZ+7HAluq6lsAST4BrAXuXqT9SdKc9ta7lb31mU+qauE3mvw6cFJV/VabfzPwc1V1ztA664B1bfZlwNfG3N3hwD9O0N0f9/pnQh+st9768fzzqpqaacFeu1qmqtYD6yfdTpINVbXG+vHt7T5Yb731kz2HZ7JYV8tsB1YMzS9vbZKkPWCxwv0rwKokRyY5ADgduHaR9iVJ2s2iDMtU1eNJzgH+lsGlkJdV1ebF2BeTD+38uNc/E/pgvfXWL7BF+UBVkrR3+Q1VSeohw12SemifDvckJyX5WpItSc7vWHtZkp1JNo257xVJbkpyd5LNSc7tWP/sJLcmuaPVv2fMfuyX5KtJPjtG7dYkdyXZmGTDGPWHJPlUknuT3JPk5zvUvqztd9ft+0nO67j/t7e/3aYkVyZ5dsf6c1vt5lH2PdMxk+SwJDck+Ua7P7Rj/Zva/p9MMuflcLPU/2n7+9+Z5DNJDulY/8etdmOS65O8qEv90LJ3JKkkh3fc/7uTbB86Dk7puv8k/679DTYn+ZOO+//k0L63Jtk4W/0c21id5OZdz6Mkx3asf0WS/92ei3+V5Cfm6sPIqmqfvDH4oPabwL8ADgDuAI7qUP8a4Bhg05j7Xwoc06afB3y94/4DPLdN7w/cAhw3Rj/+A/Bx4LNj1G4FDp/g3+By4Lfa9AHAIRP8Wz7I4AsZo9YsA+4DDmrzVwG/2aH+5cAm4DkMLiz4PPCTXY8Z4E+A89v0+cB7O9b/NIMv8X0RWDPG/l8HLGnT7x1j/z8xNP3vgT/rUt/aVzC4eOL+uY6nWfb/buA/jvhvNlP9L7V/uwPb/Au79n9o+fuBPxyjD9cDJ7fpU4Avdqz/CvCLbfptwB+PehzPdduXz9z//08cVNUPgV0/cTCSqvoS8J1xd15VO6rq9jb9CHAPg8AZtb6q6gdtdv926/TpdpLlwOuBD3epWwhJns/gQL0UoKp+WFXfG3NzJwLfrKr7O9YtAQ5KsoRBSP9Dh9qfBm6pqker6nHg74Bfm6tglmNmLYMXOdr9qV3qq+qeqhrp29mz1F/f+g9wM4PvlHSp//7Q7MHMcQzO8Zy5GPj9uWrnqR/JLPW/C1xUVY+1dXaOs/8kAU4DrhyjDwXsOtt+PnMch7PUvxT4Upu+AfjXc/VhVPtyuC8DHhia30aHcF1ISVYCRzM4++5St197G7gTuKGqOtUD/5XBk+rJjnW7FHB9ktsy+DmILo4EpoE/b8NCH05y8Jj9OJ15nlS7q6rtwPuAvwd2AA9X1fUdNrEJ+IUkL0jyHAZnXCvmqZnJEVW1o00/CBwxxjYWytuAv+5alOTCJA8AvwH8YcfatcD2qrqj636HnNOGhi6ba1hrFi9l8O94S5K/S/KzY/bhF4CHquobY9SeB/xp+xu+D7igY/1mfnRi+ibGOw6fZl8O92eEJM8FPg2ct9tZ0Lyq6omqWs3gbOvYJC/vsN9fBXZW1W2dOvxUr66qYxj8eufZSV7ToXYJg7eXl1TV0cD/YTAs0UkGX3J7A/A/O9YdyuAJcSTwIuDgJP9m1PqquofBMMb1wN8AG4EnuvRhhm0WHd99LZQk7wIeB67oWltV76qqFa32nPnWH9rnc4B30vEFYTeXAC8BVjN4kX5/x/olwGHAccDvAVe1s/CuzqDjCcaQ3wXe3v6Gb6e9m+3gbcC/TXIbgyHeH47Zj6fYl8N9r//EQZL9GQT7FVV19bjbacMZNwEndSh7FfCGJFsZDEmdkOQvOu53e7vfCXyGwVDXqLYB24bebXyKQdh3dTJwe1U91LHul4H7qmq6qv4JuBr4V102UFWXVtUrq+o1wHcZfG7S1UNJlgK0+1mHBRZLkt8EfhX4jfYCM64r6DYk8BIGL653tONwOXB7kn826gaq6qF2kvMk8CG6HYMwOA6vbsOctzJ4Fzvrh7ozacN6vwZ8suO+dzmTwfEHg5OUTo+hqu6tqtdV1SsZvMB8c8x+PMW+HO579ScO2tnBpcA9VfWBMeqndl3ZkOQg4LXAvaPWV9UFVbW8qlYyeOxfqKqRz1yTHJzkebumGXwwN/KVQ1X1IPBAkpe1phMZ7yedxz1j+nvguCTPaf8WJzL43GNkSV7Y7l/M4Mn98TH6cS2DJzft/poxtjG2DP5TnN8H3lBVj45Rv2podi3djsG7quqFVbWyHYfbGFxk8GCH/S8dmn0jHY7B5i8ZfKhKkpcy+GC/6y8s/jJwb1Vt61i3yz8Av9imTwA6De0MHYfPAv4T8Gdj9uOpFuJT2b11YzBO+nUGr3Tv6lh7JYO3gf/E4KA8q2P9qxm8Bb+TwVv6jcApHer/JfDVVr+JeT6ln2dbx9PxahkGVxnd0W6bu/792jZWAxvaY/hL4NCO9QcD3waeP+bjfg+DMNoEfIx2xUSH+v/F4AXpDuDEcY4Z4AXAjQye0J8HDutY/8Y2/RjwEPC3Heu3MPjsadcxONfVLjPVf7r9/e4E/gpYNu5zhnmuvppl/x8D7mr7vxZY2rH+AOAv2mO4HTiha/+BjwC/M+IxM1MfXg3c1o6jW4BXdqw/l0GOfR24iPbLAZPe/PkBSeqhfXlYRpI0C8NdknrIcJekHjLcJamHDHdJ6iHDXZJ6yHCXpB76f9/0Zck+JJgwAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRAINING "
      ],
      "metadata": {
        "id": "we3vsX7p4iv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(clf, X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Trains a classifier over train data and prints \n",
        "    the evaluation on val data.\"\"\"\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    prediction = clf.predict(X_val)\n",
        "\n",
        "    print('Eval f_micro is {:10.4f}'.format(\n",
        "        f1_score(y_val, prediction, average='micro')))\n",
        "    print('Eval f_macro is {:10.4f}'.format(\n",
        "        f1_score(y_val, prediction, average='macro')))"
      ],
      "metadata": {
        "id": "JDKKdt0V11oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test, dev = load_datasets(1)\n",
        "\n",
        "X_train, y_train, X_test, y_test, X_dev, y_dev = process_dataset(\n",
        "    train, test, dev)\n",
        "\n",
        "print(\"Train features shape:\", X_train.shape)\n",
        "print(\"Train labels shape:\", y_train.shape)\n",
        "\n",
        "print(\"\\nTest features shape:\", X_test.shape)\n",
        "print(\"Test labels shape:\", y_test.shape)\n",
        "\n",
        "print(\"\\nDev features shape:\", X_dev.shape)\n",
        "print(\"Dev labels shape:\", y_dev.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO4tGyS8Pcb2",
        "outputId": "01d94d5a-0740-41b9-d8c0-6c34f4777385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train features shape: (688, 364)\n",
            "Train labels shape: (688, 20)\n",
            "\n",
            "Test features shape: (200, 364)\n",
            "Test labels shape: (200, 20)\n",
            "\n",
            "Dev features shape: (63, 364)\n",
            "Dev labels shape: (63, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = [\n",
        "    DummyClassifier(strategy=\"uniform\"),\n",
        "    DummyClassifier(strategy=\"stratified\"),\n",
        "    OneVsRestClassifier(MultinomialNB(fit_prior=True, class_prior=None)),\n",
        "    OneVsRestClassifier(LinearSVC()),\n",
        "]\n",
        "\n",
        "for clf in classifiers:\n",
        "    print(clf)\n",
        "    \n",
        "    train_classifier(clf, X_train, y_train, X_test, y_test)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGXjkWQv6m_Q",
        "outputId": "0c1264d2-8ec6-4d51-ba1c-01748cd10019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DummyClassifier(strategy='uniform')\n",
            "Eval f_micro is     0.1497\n",
            "Eval f_macro is     0.1246\n",
            "\n",
            "DummyClassifier(strategy='stratified')\n",
            "Eval f_micro is     0.2595\n",
            "Eval f_macro is     0.0648\n",
            "\n",
            "OneVsRestClassifier(estimator=MultinomialNB())\n",
            "Eval f_micro is     0.2637\n",
            "Eval f_macro is     0.0381\n",
            "\n",
            "OneVsRestClassifier(estimator=LinearSVC())\n",
            "Eval f_micro is     0.3154\n",
            "Eval f_macro is     0.0942\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TASK 3"
      ],
      "metadata": {
        "id": "zDRCEnT2SNSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, test, dev = load_datasets(1)\n",
        "\n",
        "X_train, y_train, X_test, y_test, X_dev, y_dev = process_dataset(\n",
        "    train, test, dev)\n",
        "\n",
        "print(\"Train features shape:\", X_train.shape)\n",
        "print(\"Train labels shape:\", y_train.shape)\n",
        "\n",
        "print(\"\\nTest features shape:\", X_test.shape)\n",
        "print(\"Test labels shape:\", y_test.shape)\n",
        "\n",
        "print(\"\\nDev features shape:\", X_dev.shape)\n",
        "print(\"Dev labels shape:\", y_dev.shape)"
      ],
      "metadata": {
        "id": "C367nh9NSR4g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd631d3c-182a-4153-9af2-702852ee3ac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train features shape: (688, 364)\n",
            "Train labels shape: (688, 20)\n",
            "\n",
            "Test features shape: (200, 364)\n",
            "Test labels shape: (200, 20)\n",
            "\n",
            "Dev features shape: (63, 364)\n",
            "Dev labels shape: (63, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = [\n",
        "    DummyClassifier(strategy=\"uniform\"),\n",
        "    DummyClassifier(strategy=\"stratified\"),\n",
        "    OneVsRestClassifier(MultinomialNB(fit_prior=True, class_prior=None)),\n",
        "    OneVsRestClassifier(LinearSVC()),\n",
        "]\n",
        "\n",
        "for clf in classifiers:\n",
        "    print(clf)\n",
        "    \n",
        "    train_classifier(clf, X_train, y_train, X_test, y_test)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmYzjPmJS7yN",
        "outputId": "e9bccff5-905b-4ba4-fa1e-6caee90f6224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DummyClassifier(strategy='uniform')\n",
            "Eval f_micro is     0.1359\n",
            "Eval f_macro is     0.1148\n",
            "\n",
            "DummyClassifier(strategy='stratified')\n",
            "Eval f_micro is     0.2403\n",
            "Eval f_macro is     0.0680\n",
            "\n",
            "OneVsRestClassifier(estimator=MultinomialNB())\n",
            "Eval f_micro is     0.2637\n",
            "Eval f_macro is     0.0381\n",
            "\n",
            "OneVsRestClassifier(estimator=LinearSVC())\n",
            "Eval f_micro is     0.3154\n",
            "Eval f_macro is     0.0942\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TASK 2"
      ],
      "metadata": {
        "id": "hokvuN5lGwEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DATASET"
      ],
      "metadata": {
        "id": "ExUfKErPGyuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Span():\n",
        "\n",
        "\n",
        "    def __init__(self, start=-1, end=-1):\n",
        "\n",
        "        if start > end: \n",
        "            start = -1 \n",
        "            end = -1\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "\n",
        "    def is_valid(self) -> bool: \n",
        "\n",
        "        return self.start >= 0 and self.end >= 0 and self.start <= self.end\n",
        "\n",
        "    def intersect(self, other):\n",
        "\n",
        "        if not self.is_valid() or not other.is_valid(): \n",
        "            return Span()\n",
        "        return Span(max(self.start, other.start), min(self.end, other.end))\n",
        "\n",
        "    def length(self) -> int: \n",
        "\n",
        "        if not self.is_valid(): \n",
        "            return 0\n",
        "        return self.end - self.start\n",
        "\n",
        "    def __eq__(self, __o: object) -> bool:\n",
        "\n",
        "        return self.start == __o.start and self.end == __o.end\n",
        "\n",
        "    def __lt__(self, __o: object) -> bool:\n",
        "\n",
        "        if self.start == __o.start: \n",
        "            return ((self.end) < (__o.end))\n",
        "        return ((self.start) < (__o.start))\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "\n",
        "        return \"({}, {})\".format(self.start, self.end)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "\n",
        "        return \"({}, {})\".format(self.start, self.end)"
      ],
      "metadata": {
        "id": "tH6QyjDoESSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset():\n",
        "\n",
        "\n",
        "    def __init__(self, X, y, pos_map, sen_map, y_spans):\n",
        "\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        self.pos_map = pos_map\n",
        "        self.sen_map = sen_map\n",
        "\n",
        "        self.y_spans = y_spans\n",
        "\n",
        "\n",
        "def get_sentence_data(sentence, label_dict, i):\n",
        "\n",
        "    def get_word_data(m, i):\n",
        "\n",
        "        tag = [0] * N_LABELS\n",
        "        for d in label_dict:\n",
        "            if d['start'] <= m.start() and d['end'] >= m.end():\n",
        "                tag[LABELS[d['technique']]] = 1\n",
        "\n",
        "        return {'sentence': i, \n",
        "                'word': m.group().lower(), \n",
        "                'tag': tag, \n",
        "                'span': Span(m.start(), m.end())}\n",
        "\n",
        "    iterator = re.finditer(r'\\w+|[^\\w\\s]+', sentence)\n",
        "    return [get_word_data(match, i) for match in iterator]\n",
        "\n",
        "def process_dataset(train, test, dev):\n",
        "\n",
        "    def process(data):\n",
        "\n",
        "        def label_dicts_to_spans(label_dicts):\n",
        "\n",
        "            spans = [[] for _ in range(N_LABELS)]\n",
        "            for d in label_dicts: \n",
        "                t_num = LABELS[d['technique']]\n",
        "                spans[t_num].append(Span(d['start'], d['end']))\n",
        "\n",
        "            return spans\n",
        "\n",
        "        sentences = data.text\n",
        "        label_dicts = data.labels\n",
        "\n",
        "        res = []\n",
        "        for i, [s, l] in enumerate(zip(sentences, label_dicts)):\n",
        "            res.extend(get_sentence_data(s, l, i))\n",
        "\n",
        "        spans = [label_dicts_to_spans(ds) for ds in label_dicts]\n",
        "\n",
        "        return pd.DataFrame(res), spans\n",
        "\n",
        "    train, train_spans = process(train)\n",
        "    test, test_spans = process(test)\n",
        "    dev, dev_spans = process(dev)\n",
        "\n",
        "    df = pd.concat([train, test, dev])\n",
        "\n",
        "    X = df.drop(['tag', 'span'], axis=1)\n",
        "    v = DictVectorizer(sparse=False)\n",
        "    X = v.fit_transform(X.to_dict('records'))\n",
        "    y = np.stack(df.tag.values)\n",
        "\n",
        "    pos_map = np.stack(df.span.values)\n",
        "    sen_map = np.stack(df.sentence.values)\n",
        "\n",
        "    l_train = len(train)\n",
        "    l_test = len(test)\n",
        "\n",
        "    def make_dataset(s, e, spans): \n",
        "        return Dataset(X[s:e], y[s:e], pos_map[s:e], sen_map[s:e], spans)\n",
        "\n",
        "    train = make_dataset(0, l_train, train_spans)\n",
        "    test = make_dataset(l_train, l_train+l_test, test_spans)\n",
        "    dev = make_dataset(l_train+l_test, len(X), dev_spans)\n",
        "\n",
        "    return train, test, dev"
      ],
      "metadata": {
        "id": "x20naEnOG290"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRAINING"
      ],
      "metadata": {
        "id": "TIP8z703Gvrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, test, dev = load_datasets(2)\n",
        "\n",
        "train, test, dev = process_dataset(train, test, dev)\n",
        "\n",
        "print(\"Train features shape:\", train.X.shape)\n",
        "print(\"Train labels shape:\", train.y.shape)\n",
        "\n",
        "print(\"\\nTest features shape:\", test.X.shape)\n",
        "print(\"Test labels shape:\", test.y.shape)\n",
        "\n",
        "print(\"\\nDev features shape:\", dev.X.shape)\n",
        "print(\"Dev labels shape:\", dev.y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjfllyTY5TRf",
        "outputId": "07fb78c2-822b-410f-b2bb-62a9a5bbc875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train features shape: (14760, 3927)\n",
            "Train labels shape: (14760, 20)\n",
            "\n",
            "Test features shape: (4083, 3927)\n",
            "Test labels shape: (4083, 20)\n",
            "\n",
            "Dev features shape: (1406, 3927)\n",
            "Dev labels shape: (1406, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = [\n",
        "    DummyClassifier(strategy=\"uniform\"),\n",
        "    DummyClassifier(strategy=\"stratified\"),\n",
        "    OneVsRestClassifier(MultinomialNB(fit_prior=True, class_prior=None)),\n",
        "    OneVsRestClassifier(LinearSVC()),\n",
        "]\n",
        "\n",
        "for clf in classifiers:\n",
        "    print(clf)\n",
        "\n",
        "    train_classifier(clf, train.X, train.y, test.X, test.y)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weLbe71sKEng",
        "outputId": "31505e02-0338-49b7-8b13-b10cb088e72e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DummyClassifier(strategy='uniform')\n",
            "Eval f_micro is     0.0809\n",
            "Eval f_macro is     0.0731\n",
            "\n",
            "DummyClassifier(strategy='stratified')\n",
            "Eval f_micro is     0.1173\n",
            "Eval f_macro is     0.0406\n",
            "\n",
            "OneVsRestClassifier(estimator=MultinomialNB())\n",
            "Eval f_micro is     0.0612\n",
            "Eval f_macro is     0.0223\n",
            "\n",
            "OneVsRestClassifier(estimator=LinearSVC())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval f_micro is     0.0742\n",
            "Eval f_macro is     0.0331\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TESTING"
      ],
      "metadata": {
        "id": "Gt_jDxVMMHnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ohe_to_spans(ohe, pos_map, sen_map):\n",
        "\n",
        "    def in_sentence_groups(x_list, sen_map):\n",
        "        '''Returns \"x_list\" regrouped as in \"sen_map\".'''\n",
        "\n",
        "        d = {}\n",
        "\n",
        "        def add(x, group):\n",
        "            \n",
        "            if group in d:\n",
        "                d[group].append(x)\n",
        "            else:\n",
        "                d[group] = [x]\n",
        "\n",
        "        for x, s in zip(x_list, sen_map):\n",
        "            add(x, s)\n",
        "        \n",
        "        return [np.stack(x) for x in d.values()]\n",
        "\n",
        "    def ohe_sentence_to_spans(ohe_sentence, pos_map_sentence):\n",
        "\n",
        "        results = []\n",
        "        for class_res in ohe_sentence.T:\n",
        "            result = []\n",
        "            cur_span = None\n",
        "            for tok_res, tok_span in zip(class_res, pos_map_sentence):\n",
        "                if cur_span is None:\n",
        "                    if tok_res == 1:\n",
        "                        cur_span = tok_span\n",
        "                else:\n",
        "                    if tok_res == 1:\n",
        "                        cur_span.end = tok_span.end\n",
        "                    if tok_res == 0:\n",
        "                        result.append(cur_span)\n",
        "                        cur_span = None\n",
        "\n",
        "            if cur_span is not None: result.append(cur_span)\n",
        "            results.append(result)\n",
        "            \n",
        "        return results\n",
        "\n",
        "    return [\n",
        "        ohe_sentence_to_spans(ohe, pos) for ohe, pos in zip(\n",
        "            in_sentence_groups(ohe, sen_map),\n",
        "            in_sentence_groups(pos_map, sen_map))]"
      ],
      "metadata": {
        "id": "I4Ogj3YV_F-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def C(s, t, h): return s.intersect(t).length() / h\n",
        "\n",
        "def precision_metric(S, T):\n",
        "\n",
        "    Sn = 0\n",
        "    sum = 0\n",
        "    for c_S, c_T in zip(S, T):\n",
        "        for s in c_S:\n",
        "            for t in c_T: \n",
        "                sum += C(s, t, s.length())\n",
        "\n",
        "        Sn += len(c_S)\n",
        "\n",
        "    if Sn == 0 and sum == 0: \n",
        "        return 1\n",
        "    if Sn == 0: \n",
        "        return 0\n",
        "\n",
        "    return sum / Sn\n",
        "\n",
        "def recall_metric(S, T): return precision_metric(T, S)\n",
        "\n",
        "def f1_metric(pred_spans, label_spans):\n",
        "\n",
        "    p_sum = 0\n",
        "    r_sum = 0\n",
        "    for S, T in zip(pred_spans, label_spans):\n",
        "        p_sum += precision_metric(S, T)\n",
        "        r_sum += recall_metric(S, T)\n",
        "\n",
        "    p = p_sum / len(pred_spans)\n",
        "    r = r_sum / len(pred_spans)\n",
        "\n",
        "    return 2 * (p*r) / (p+r), p, r\n",
        "\n",
        "def test_classifier(clf, X_test, y_test_spans, pos_map, sen_map):\n",
        "    \"\"\"Tests a classifier using test data and labels in the\n",
        "    form of spans.\"\"\"\n",
        "\n",
        "    y_pred_ohe = clf.predict(X_test)\n",
        "\n",
        "    y_pred_spans = ohe_to_spans(y_pred_ohe, pos_map, sen_map)\n",
        "\n",
        "    f1, p, r = f1_metric(y_pred_spans, y_test_spans)\n",
        "\n",
        "    print(clf)\n",
        "    print('Test f1 is {:10.4f}'.format(f1))\n",
        "    print('Test precision is {:10.4f}'.format(p))\n",
        "    print('Test recall is {:10.4f}'.format(r))\n",
        "    print()"
      ],
      "metadata": {
        "id": "_jhMQfqhPtiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for clf in classifiers:\n",
        "    test_classifier(\n",
        "        clf, test.X, test.y_spans, test.pos_map, test.sen_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMZXNoj5H9Gj",
        "outputId": "2c6bd363-4d99-4e69-a890-3b6360288921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DummyClassifier(strategy='uniform')\n",
            "Test f1 is     0.0826\n",
            "Test precision is     0.0426\n",
            "Test recall is     1.3069\n",
            "\n",
            "DummyClassifier(strategy='stratified')\n",
            "Test f1 is     0.1649\n",
            "Test precision is     0.0996\n",
            "Test recall is     0.4791\n",
            "\n",
            "OneVsRestClassifier(estimator=MultinomialNB())\n",
            "Test f1 is     0.4213\n",
            "Test precision is     0.5729\n",
            "Test recall is     0.3332\n",
            "\n",
            "OneVsRestClassifier(estimator=LinearSVC())\n",
            "Test f1 is     0.2577\n",
            "Test precision is     0.1952\n",
            "Test recall is     0.3791\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "nO4a3weHOzd7",
        "WhiCUghlw8Ew",
        "dDpu3qtRulP6",
        "zDRCEnT2SNSA",
        "hokvuN5lGwEj",
        "ExUfKErPGyuu",
        "TIP8z703Gvrv",
        "Gt_jDxVMMHnD"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}